%%%%%%%%%%%%%%% Parallel methods %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Parallel Methods}
The parallel code takes advantage of the GPU's unique computing capability by
completely restructuring the calculation strategy. Instead of looping through
every particle and tracking it through space, the parallel code assigns a thread
to every voxel in space. Each thread performs a line-box intersection qeuery,
tallying the particle contribution where appropriate\cite{ray_trace}. Every
thread must check every particle track. 

The line-box intersection query is used to check if the particle track passes 
through the voxel. If the particle track crossed the voxel, a particle contribution 
is tallied. In this case, the particle contribution is quantified as flux. 

\subsection{Block dimensions and grid dimension set-up}
To set up the grid dimensions and block dimensions, we first considered the max number of 
threads per dimension in a 3D block. 
The grid dimensions were chosen to be a 1D array of blocks. The number of blocks 
needed per simulation was calculated using the dimensions of the mesh and the 
dimensions of the block. This allowed us to launch one thread per voxel, while 
minimizing the number the number of blocks per grid. 

\subsection{Memory Issues}
For this algorithm, we were forced to use global memory instead of shared memory. 
Unfortunately we were limited by the amount of shared memory we 
could access with one kernel call. In the type of problems we were addressing, 
we typically encounter large numbers of voxels in the geometry mesh which
requiring 
large memory allocation. For example, a 40 x 40 x 40 mesh is 64,000 
voxels, if we store a float in each voxel, we need 
256 kB of shared memory which is over-budget for shared memory per multiprocessor.   

The philosophy of this approach is to make the compute time independent of
particle mean free path. Since every thread checks every particle at one point
in space, the compute time is a fixed function of mesh size and the number of
problem particles. Even if the particles traverse a large number of voxels, The
same 6 calculations are performed per particle, per voxel.
