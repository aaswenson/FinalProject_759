%%%%%%%%%%%%%%% Parallel methods %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Parallel Methods}
The parallel code takes advantage of the GPU's unique computing capability by
completely restructuring the calculation strategy. Instead of looping through
every particle and tracking it through space, the parallel code assigns a thread
to every voxel in space. Each thread performs a line-box intersection qeuery,
tallying the particle contribution where appropriate. Every thread must check
every particle track. 

The line-box intersection query is used to check if the particle track passes 
through the voxel. If the particle track crossed the voxel, a particle contribution 
is tallied. In this case, the particle contribution is quantified as flux. 

\subsection{Block dimensions and grid dimension set-up}
To set up the grid dimenions and block dimensions, we first considered the max number of 
threads per dimension in a 3D block. 
The grid dimesions were chosen to be a 1D array of blocks. The number of blocks 
needed per simulation was calculated using the dimesions of the mesh and the 
dimensions of the block. This allowed us to launch one thread per voxel, while 
minimizing the number the number of blocks per grid. 

\subsection{Memory Issues}
For this algorithm, we decided to use global memory instead of shared memory. 
The reason for this is we found limitations on the amount of shared memory we 
could access with one kernel call. In the type of problems we were addressing, 
we typically find large number of voxels in the geometry mesh which would require 
large memory allocation. For example having a 40 x 40 x 40 mesh means 64,000 
voxels, if we want to store a float in each one of these voxels, we would need 
256 kB which is over the amount of shared memory we can have per multiprocessor.   


The philosophy of this approach is to make the compute time independent of
particle mean free path. Since every thread checks every particle at one point
in space, the compute time is a fixed function of mesh size and the number of
problem particles. Even if the particles traverse a large number of voxels, The
same 6 calculations are performed per particle, per voxel.

